#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Skill-Zero Analyzer - Data Processor
ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã¨ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«URLã‹ã‚‰å‚åŠ è€…ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import pandas as pd
import json
import requests
from bs4 import BeautifulSoup
import time
import re
from urllib.parse import urlparse
import os
import glob
from dotenv import load_dotenv
from typing import Dict, List, Optional, Any
from config import config
from utils import Logger, FileUtils, HttpUtils, DataUtils, ValidationUtils


class ProfileExtractor:
    """ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æƒ…å ±æŠ½å‡ºã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.logger = Logger.setup_logger(__name__)
        self.session = HttpUtils.create_session()

    def extract_profile_from_url(self, url: str) -> Optional[Dict[str, Any]]:
        """URLã‹ã‚‰ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æƒ…å ±ã‚’æŠ½å‡º"""
        if not ValidationUtils.is_valid_url(url):
            return None

        try:
            if "libecity.com/user_profile/" in url:
                return self._extract_libecity_profile(url)
            else:
                self.logger.warning(f"æœªå¯¾å¿œã®URLå½¢å¼: {url}")
                return None
        except Exception as e:
            self.logger.error(f"URLå‡¦ç†ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
            return None

    def _extract_libecity_profile(self, url: str) -> Optional[Dict[str, Any]]:
        """ãƒªãƒ™ã‚·ãƒ†ã‚£ã®ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º"""
        try:
            # å¸¸ã«ãƒ­ã‚°ã‚¤ãƒ³ã‚’å®Ÿè¡Œ
            self._login_to_libecity()

            response = self._fetch_profile_page(url)
            if not response:
                self.logger.warning(f"ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—: {url}")
                return None

            profile_info = self._parse_profile_html(response, url)
            if profile_info:
                return profile_info
            else:
                self.logger.warning(f"ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æƒ…å ±ã®è§£æã«å¤±æ•—: {url}")
                return None

        except Exception as e:
            self.logger.error(f"ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æŠ½å‡ºã‚¨ãƒ©ãƒ¼ ({url}): {e}")
            return None

    def _login_to_libecity(self) -> None:
        """ãƒªãƒ™ã‚·ãƒ†ã‚£ã«ãƒ­ã‚°ã‚¤ãƒ³"""
        try:
            self.logger.info("ãƒªãƒ™ã‚·ãƒ†ã‚£ã¸ã®ãƒ­ã‚°ã‚¤ãƒ³ã‚’é–‹å§‹ã—ã¾ã™...")

            # ã¾ãšãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’åˆæœŸåŒ–
            main_response = HttpUtils.safe_request(self.session, "https://libecity.com")
            if not main_response:
                self.logger.error("ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return

            # ã‚µã‚¤ãƒ³ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
            signin_url = "https://libecity.com/signin"
            signin_response = HttpUtils.safe_request(self.session, signin_url)
            if not signin_response:
                self.logger.error("ã‚µã‚¤ãƒ³ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return

            signin_soup = BeautifulSoup(signin_response.content, 'html.parser')
            self.logger.info(f"ã‚µã‚¤ãƒ³ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«: {signin_soup.find('title').get_text() if signin_soup.find('title') else 'ã‚¿ã‚¤ãƒˆãƒ«ãªã—'}")

            # CSRFãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŠ½å‡º
            csrf_token = self._extract_csrf_token(signin_soup)
            if csrf_token:
                self.logger.info(f"CSRFãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—: {csrf_token[:20]}...")

            # ãƒ­ã‚°ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
            login_data = {
                'email': 'eleven9terror@gmail.com',
                'password': 'bluearms109'
            }
            if csrf_token:
                login_data['_token'] = csrf_token

            self.logger.info("ãƒ­ã‚°ã‚¤ãƒ³ã‚’è©¦è¡Œã—ã¾ã™...")
            login_response = self.session.post(signin_url, data=login_data, timeout=config.REQUEST_TIMEOUT, allow_redirects=True)

            self.logger.info(f"ãƒ­ã‚°ã‚¤ãƒ³è©¦è¡Œçµæœ: {login_response.status_code}")
            self.logger.info(f"æœ€çµ‚URL: {login_response.url}")

            # ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸã®ç¢ºèª
            if login_response.status_code in [200, 302]:
                # ãƒ­ã‚°ã‚¤ãƒ³å¾Œã®ãƒšãƒ¼ã‚¸å†…å®¹ã‚’ç¢ºèª
                login_result_soup = BeautifulSoup(login_response.content, 'html.parser')

                # ãƒ­ã‚°ã‚¢ã‚¦ãƒˆãƒªãƒ³ã‚¯ã®å­˜åœ¨ã‚’ç¢ºèª
                logout_link = login_result_soup.find('a', href=lambda x: x and 'logout' in x)
                if logout_link:
                    self.logger.info("ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸã‚’ç¢ºèª: ãƒ­ã‚°ã‚¢ã‚¦ãƒˆãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
                else:
                    self.logger.warning("ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ã‚’ç¢ºèªã§ãã¾ã›ã‚“ã§ã—ãŸ")

                # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¯ãƒƒã‚­ãƒ¼ã‚’ç¢ºèª
                self.logger.info(f"ãƒ­ã‚°ã‚¤ãƒ³å¾Œã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¯ãƒƒã‚­ãƒ¼: {dict(self.session.cookies)}")

                # ãƒã‚¤ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ã‚’ç¢ºèª
                mypage_response = HttpUtils.safe_request(self.session, "https://libecity.com/mypage/home")
                if mypage_response:
                    mypage_soup = BeautifulSoup(mypage_response.content, 'html.parser')
                    mypage_title = mypage_soup.find('title')
                    if mypage_title:
                        self.logger.info(f"ãƒã‚¤ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«: {mypage_title.get_text()}")

                    # ãƒã‚¤ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚Œã°ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸ
                    if 'mypage' in mypage_response.url.lower():
                        self.logger.info("ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸ: ãƒã‚¤ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã—ãŸ")
                    else:
                        self.logger.warning("ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ãŒä¸æ˜ã§ã™")
                else:
                    self.logger.warning("ãƒã‚¤ãƒšãƒ¼ã‚¸ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸ")
            else:
                self.logger.error("ãƒ­ã‚°ã‚¤ãƒ³ã«å¤±æ•—ã—ã¾ã—ãŸ")

        except Exception as e:
            self.logger.error(f"ãƒ­ã‚°ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")

    def _extract_csrf_token(self, soup: BeautifulSoup) -> Optional[str]:
        """CSRFãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŠ½å‡º"""
        csrf_input = soup.find('input', {'name': '_token'})
        if csrf_input:
            return csrf_input.get('value')
        return None

    def _fetch_profile_page(self, url: str) -> Optional[requests.Response]:
        """ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã‚’å–å¾—"""
        try:
            self.logger.info(f"ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹: {url}")

            response = HttpUtils.safe_request(self.session, url)
            if not response:
                return None

            # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã‹ã©ã†ã‹ã‚’ç¢ºèª
            if not self._is_valid_profile_page(response):
                self._save_debug_info(response, url)
                return None

            return response

        except Exception as e:
            self.logger.error(f"ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def _is_valid_profile_page(self, response: requests.Response) -> bool:
        """ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã‹ã©ã†ã‹ã‚’ç¢ºèª"""
        soup = BeautifulSoup(response.content, 'html.parser')

        # HTMLã®å†…å®¹ã‚’ãƒ­ã‚°ã«å‡ºåŠ›ã—ã¦ãƒ‡ãƒãƒƒã‚°
        title = soup.find('title')
        title_text = title.get_text() if title else 'ã‚¿ã‚¤ãƒˆãƒ«ãªã—'
        self.logger.info(f"HTMLã‚¿ã‚¤ãƒˆãƒ«: {title_text}")

        # ã‚ˆã‚ŠæŸ”è»Ÿãªåˆ¤å®šæ¡ä»¶
        profile_indicators = [
            # ã‚¿ã‚¤ãƒˆãƒ«ã«ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãŒå«ã¾ã‚Œã‚‹
            lambda: title and 'ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«' in title_text,
            # content_titleã‚¯ãƒ©ã‚¹ãŒå­˜åœ¨ã™ã‚‹
            lambda: soup.find('h3', class_='content_title') is not None,
            # è‡ªå·±ç´¹ä»‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒå­˜åœ¨ã™ã‚‹
            lambda: soup.find('dt', string='è‡ªå·±ç´¹ä»‹') is not None,
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼åãŒå­˜åœ¨ã™ã‚‹
            lambda: soup.find('h3', class_='content_title') is not None,
            # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«é–¢é€£ã®è¦ç´ ãŒå­˜åœ¨ã™ã‚‹
            lambda: soup.find('dt') is not None and soup.find('dd') is not None,
            # ãƒ­ã‚°ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã§ãªã„ã“ã¨ã‚’ç¢ºèª
            lambda: 'signin' not in response.url.lower() and 'ãƒ­ã‚°ã‚¤ãƒ³' not in soup.get_text(),
            # ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ã§ãªã„ã“ã¨ã‚’ç¢ºèª
            lambda: 'error' not in soup.get_text().lower() and 'ã‚¨ãƒ©ãƒ¼' not in soup.get_text(),
            # èªè¨¼ãŒå¿…è¦ãªãƒšãƒ¼ã‚¸ã§ãªã„ã“ã¨ã‚’ç¢ºèª
            lambda: 'èªè¨¼' not in soup.get_text() and 'auth' not in response.url.lower(),
        ]

        # ã„ãšã‚Œã‹ã®æ¡ä»¶ã‚’æº€ãŸã›ã°ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã¨ã¿ãªã™
        for i, indicator in enumerate(profile_indicators):
            try:
                if indicator():
                    self.logger.info(f"ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã‚’ç¢ºèªã—ã¾ã—ãŸ (æ¡ä»¶{i+1})")
                    return True
            except Exception as e:
                self.logger.warning(f"ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«åˆ¤å®šã‚¨ãƒ©ãƒ¼ (æ¡ä»¶{i+1}): {e}")
                continue

        # ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ã¯è­¦å‘Šã¨ã—ã¦è¨˜éŒ²ã™ã‚‹ãŒã€åˆ¤å®šã‹ã‚‰ã¯é™¤å¤–
        if len(response.content) < config.MIN_PROFILE_SIZE:
            self.logger.warning(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚µã‚¤ã‚ºãŒå°ã•ã„: {len(response.content)} bytes")

        # æœ€å¾Œã®æ‰‹æ®µã¨ã—ã¦ã€HTMLã«ä½•ã‚‰ã‹ã®å†…å®¹ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if len(soup.get_text().strip()) > 100:
            self.logger.info("HTMLã«ååˆ†ãªå†…å®¹ãŒã‚ã‚‹ãŸã‚ã€ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã¨ã—ã¦å‡¦ç†ã—ã¾ã™")
            return True

        return False

    def _save_debug_info(self, response: requests.Response, url: str) -> None:
        """ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’ä¿å­˜"""
        try:
            debug_file = config.DEBUG_FILE
            FileUtils.ensure_directory(os.path.dirname(debug_file))

            with open(debug_file, 'a', encoding='utf-8') as f:
                f.write(f"\n=== {url} ===\n")
                f.write(f"Status Code: {response.status_code}\n")
                f.write(f"Content Length: {len(response.content)} bytes\n")
                f.write(f"Content Type: {response.headers.get('content-type', 'unknown')}\n")
                f.write(f"Final URL: {response.url}\n")
                f.write("=" * 50 + "\n")

            self.logger.info(f"ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¿½åŠ ã—ã¾ã—ãŸ: {debug_file}")

        except Exception as e:
            self.logger.error(f"ãƒ‡ãƒãƒƒã‚°æƒ…å ±ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def _parse_profile_html(self, response: requests.Response, url: str) -> Dict[str, Any]:
        """HTMLã‹ã‚‰ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æƒ…å ±ã‚’è§£æ"""
        soup = BeautifulSoup(response.content, 'html.parser')

        profile_info = {
            'url': url,
            'extracted_at': time.strftime('%Y-%m-%d %H:%M:%S')
        }

        # HTMLã®å†…å®¹ã‚’ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
        self.logger.info(f"HTMLå†…å®¹ã®é•·ã•: {len(soup.get_text())} æ–‡å­—")
        self.logger.info(f"HTMLè¦ç´ æ•°: {len(soup.find_all())} å€‹")

        # HTMLã®ç”Ÿã®å†…å®¹ã‚’ç¢ºèª
        raw_html = response.content.decode('utf-8', errors='ignore')
        self.logger.info(f"ç”ŸHTMLã®é•·ã•: {len(raw_html)} æ–‡å­—")
        self.logger.info(f"ç”ŸHTMLã®å…ˆé ­100æ–‡å­—: {raw_html[:100]}")

        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®å•é¡Œã‚’ç¢ºèª
        self.logger.info(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {response.encoding}")
        self.logger.info(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹Content-Type: {response.headers.get('content-type', 'unknown')}")

        # å„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®æŠ½å‡ºã‚’è©¦è¡Œ
        extraction_methods = [
            self._extract_username,
            self._extract_bio_and_related,
            self._extract_basic_info,
            self._extract_skills_and_duration
        ]

        for method in extraction_methods:
            try:
                method(soup, profile_info)
            except Exception as e:
                self.logger.warning(f"ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æŠ½å‡ºã‚¨ãƒ©ãƒ¼ ({method.__name__}): {e}")

        # æŠ½å‡ºã•ã‚ŒãŸæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
        self.logger.info(f"æŠ½å‡ºã•ã‚ŒãŸãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æƒ…å ±: {list(profile_info.keys())}")

        # æœ€ä½é™ã®æƒ…å ±ãŒã‚ã‚Œã°æˆåŠŸã¨ã™ã‚‹ï¼ˆã‚ˆã‚Šç·©ã„æ¡ä»¶ï¼‰
        if len(profile_info) > 2:  # URLã¨extracted_atä»¥å¤–ã®æƒ…å ±ãŒã‚ã‚‹
            self.logger.info(f"ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æƒ…å ±ã‚’æŠ½å‡ºã—ã¾ã—ãŸ: {url}")
            return profile_info
        else:
            # éƒ¨åˆ†çš„ãªæƒ…å ±ã§ã‚‚ä¿å­˜
            self.logger.warning(f"ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æƒ…å ±ã®æŠ½å‡ºã«éƒ¨åˆ†çš„ã«å¤±æ•—ã—ã¾ã—ãŸ: {url}")
            # åŸºæœ¬çš„ãªæƒ…å ±ã‚’è¿½åŠ 
            profile_info['partial_extraction'] = True
            profile_info['html_content_length'] = len(response.content)
            profile_info['html_text_length'] = len(soup.get_text())
            return profile_info

    def _extract_username(self, soup: BeautifulSoup, profile_info: Dict[str, Any]) -> None:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’æŠ½å‡º"""
        username_elem = soup.find('h3', class_='content_title')
        if username_elem:
            username_text = username_elem.get_text(strip=True)
            username_text = username_text.replace('ã•ã‚“ã®ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«', '').strip()
            if username_text and len(username_text) < 50:
                profile_info['username'] = username_text
                self.logger.info(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’æŠ½å‡º: {username_text}")

    def _extract_bio_and_related(self, soup: BeautifulSoup, profile_info: Dict[str, Any]) -> None:
        """è‡ªå·±ç´¹ä»‹ã¨é–¢é€£æƒ…å ±ã‚’æŠ½å‡º"""
        bio_text = DataUtils.extract_field_value(soup, 'è‡ªå·±ç´¹ä»‹')
        if bio_text:
            profile_info['bio'] = bio_text
            self.logger.info(f"è‡ªå·±ç´¹ä»‹ã‚’æŠ½å‡º: {bio_text[:50]}...")

            # è‡ªå·±ç´¹ä»‹ã‹ã‚‰é–¢é€£æƒ…å ±ã‚’æŠ½å‡º
            self._extract_work_history_from_bio(bio_text, profile_info)
            self._extract_likes_from_bio(bio_text, profile_info)
            self._extract_strengths_from_bio(bio_text, profile_info)

    def _extract_work_history_from_bio(self, bio_text: str, profile_info: Dict[str, Any]) -> None:
        """è‡ªå·±ç´¹ä»‹ã‹ã‚‰ä»•äº‹å±¥æ­´ã‚’æŠ½å‡º"""
        if 'ä»•äº‹ğŸ’¼ï¼š' in bio_text:
            work_start = bio_text.find('ä»•äº‹ğŸ’¼ï¼š')
            work_text = bio_text[work_start:]
            next_section = work_text.find('\n\n')
            if next_section != -1:
                work_text = work_text[:next_section]
            profile_info['work_history'] = work_text
            self.logger.info(f"ä»•äº‹å±¥æ­´ã‚’æŠ½å‡º: {work_text[:50]}...")

    def _extract_likes_from_bio(self, bio_text: str, profile_info: Dict[str, Any]) -> None:
        """è‡ªå·±ç´¹ä»‹ã‹ã‚‰å¥½ããªã“ã¨ã‚’æŠ½å‡º"""
        if 'å¥½ããªã“ã¨ğŸŒŸï¼š' in bio_text:
            likes_start = bio_text.find('å¥½ããªã“ã¨ğŸŒŸï¼š')
            likes_end = bio_text.find('å¾—æ„ãªã“ã¨ğŸ§ ï¼š')
            if likes_start != -1 and likes_end != -1:
                likes_text = bio_text[likes_start:likes_end].replace('å¥½ããªã“ã¨ğŸŒŸï¼š', '').strip()
                profile_info['likes'] = likes_text
                self.logger.info(f"å¥½ããªã“ã¨ã‚’æŠ½å‡º: {likes_text}")

    def _extract_strengths_from_bio(self, bio_text: str, profile_info: Dict[str, Any]) -> None:
        """è‡ªå·±ç´¹ä»‹ã‹ã‚‰å¾—æ„ãªã“ã¨ã‚’æŠ½å‡º"""
        if 'å¾—æ„ãªã“ã¨ğŸ§ ï¼š' in bio_text:
            strengths_start = bio_text.find('å¾—æ„ãªã“ã¨ğŸ§ ï¼š')
            work_start = bio_text.find('ä»•äº‹ğŸ’¼ï¼š')
            if strengths_start != -1 and work_start != -1:
                strengths_text = bio_text[strengths_start:work_start].replace('å¾—æ„ãªã“ã¨ğŸ§ ï¼š', '').strip()
                profile_info['strengths'] = strengths_text
                self.logger.info(f"å¾—æ„ãªã“ã¨ã‚’æŠ½å‡º: {strengths_text}")

    def _extract_basic_info(self, soup: BeautifulSoup, profile_info: Dict[str, Any]) -> None:
        """åŸºæœ¬æƒ…å ±ã‚’æŠ½å‡º"""
        basic_fields = [
            ('å‡ºèº«åœ°', 'birthplace'),
            ('è·ç¨®ãƒ»è·æ¥­', 'occupation'),
            ('å®¶æ—æ§‹æˆ', 'family'),
            ('ãƒªãƒ™å¤§ã¨ã®å‡ºä¼šã„', 'libecity_encounter'),
            ('æŒ‘æˆ¦ã€å®Ÿè·µã—ã¦ã„ã‚‹ã“ã¨ã€ã“ã‚Œã‹ã‚‰ã‚„ã‚ŠãŸã„ã“ã¨ãªã©', 'challenges'),
            ('è¶£å‘³ãƒ»ç‰¹æŠ€', 'hobbies'),
            ('å¥½ããªã€‡ã€‡', 'likes')
        ]

        for field_name, key in basic_fields:
            value = DataUtils.extract_field_value(soup, field_name)
            if value:
                profile_info[key] = value
                self.logger.info(f"{field_name}ã‚’æŠ½å‡º: {value[:50]}...")

    def _extract_skills_and_duration(self, soup: BeautifulSoup, profile_info: Dict[str, Any]) -> None:
        """ã‚¹ã‚­ãƒ«ã¨åœ¨ç±æœŸé–“ã‚’æŠ½å‡º"""
        # çµŒæ­´ãƒ»ã‚¹ã‚­ãƒ«
        skills_text = DataUtils.extract_field_value(soup, 'çµŒæ­´ãƒ»ã‚¹ã‚­ãƒ«')
        if skills_text:
            profile_info['skills'] = skills_text
            self.logger.info(f"çµŒæ­´ãƒ»ã‚¹ã‚­ãƒ«ã‚’æŠ½å‡º: {skills_text[:50]}...")

        # åœ¨ç±æœŸé–“
        duration_text = DataUtils.extract_field_value(soup, 'åœ¨ç±æœŸé–“')
        if duration_text:
            profile_info['duration'] = duration_text
            self.logger.info(f"åœ¨ç±æœŸé–“ã‚’æŠ½å‡º: {duration_text}")


class DataMerger:
    """ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚¯ãƒ©ã‚¹"""

    @staticmethod
    def merge_duplicate_participants(participants: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """é‡è¤‡ã™ã‚‹å‚åŠ è€…ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ"""
        merged_participants = []
        processed_names = set()

        for participant in participants:
            nickname = participant.get('nickname', '').strip()
            if not nickname:
                continue

            if nickname in processed_names:
                existing_index = DataMerger._find_existing_participant(merged_participants, nickname)
                if existing_index is not None:
                    merged = DataMerger._merge_participant_data(
                        merged_participants[existing_index], participant
                    )
                    merged_participants[existing_index] = merged
                    print(f"é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã—ã¾ã—ãŸ: {nickname}")
                else:
                    merged_participants.append(participant)
            else:
                merged_participants.append(participant)
                processed_names.add(nickname)

        return merged_participants

    @staticmethod
    def _find_existing_participant(participants: List[Dict[str, Any]], nickname: str) -> Optional[int]:
        """æ—¢å­˜ã®å‚åŠ è€…ã‚’æ¤œç´¢"""
        for i, existing in enumerate(participants):
            if existing.get('nickname', '').strip() == nickname:
                return i
        return None

    @staticmethod
    def _merge_participant_data(existing: Dict[str, Any], new: Dict[str, Any]) -> Dict[str, Any]:
        """å‚åŠ è€…ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ"""
        return DataUtils.merge_dicts(existing, new)


class DebugFileCleaner:
    """ãƒ‡ãƒãƒƒã‚°ãƒ•ã‚¡ã‚¤ãƒ«æ•´ç†ã‚¯ãƒ©ã‚¹"""

    @staticmethod
    def clean_debug_files():
        """ãƒ‡ãƒãƒƒã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ•´ç†"""
        try:
            # æ—¢å­˜ã®debug_infoã¨debug_pageãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            debug_files = glob.glob("output/debug_*")
            for file in debug_files:
                if os.path.exists(file):
                    os.remove(file)
                    print(f"å‰Šé™¤: {file}")

            # æ–°ã—ã„ãƒ‡ãƒãƒƒã‚°ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
            with open(config.DEBUG_FILE, 'w', encoding='utf-8') as f:
                f.write("=== ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚µãƒãƒªãƒ¼ ===\n")
                f.write(f"ä½œæˆæ—¥æ™‚: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write("=" * 50 + "\n")

            print("ãƒ‡ãƒãƒƒã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ•´ç†ã—ã¾ã—ãŸ")

        except Exception as e:
            print(f"ãƒ‡ãƒãƒƒã‚°ãƒ•ã‚¡ã‚¤ãƒ«æ•´ç†ã‚¨ãƒ©ãƒ¼: {e}")


class DataProcessor:
    """ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        # ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
        load_dotenv()
        self.logger = Logger.setup_logger(__name__)
        self.profile_extractor = ProfileExtractor()

    def load_csv_data(self) -> Optional[pd.DataFrame]:
        """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        try:
            df = pd.read_csv(config.CSV_FILE)
            self.logger.info(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿")
            return df
        except FileNotFoundError:
            self.logger.error(f"ã‚¨ãƒ©ãƒ¼: {config.CSV_FILE} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
        except Exception as e:
            self.logger.error(f"CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def process_participant_data(self, row: pd.Series) -> Dict[str, Any]:
        """å‚åŠ è€…ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†"""
        participant = {
            'timestamp': row.get('ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—', ''),
            'email': row.get('ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹', ''),
            'nickname': row.get('ãƒ‹ãƒƒã‚¯ãƒãƒ¼ãƒ \nãƒªãƒ™ã‚·ãƒ†ã‚£ã§ä½¿ç”¨ã—ã¦ã„ã‚‹åå‰ï¼‰', ''),
            'profile_url': row.get('ãƒªãƒ™ã‚·ãƒ†ã‚£ã®\nãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«URL', ''),
            'form_data': {
                'experience': row.get('ä»Šã¾ã§ã‚„ã£ã¦ããŸã“ã¨ ï¼ˆä»•äº‹ï¼ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆï¼‰', ''),
                'strengths': row.get('å¾—æ„ã¨è¨€ã‚ã‚ŒãŸã“ã¨ï¼å¥½ããªã“ã¨', ''),
                'appreciation': row.get('äººã«æ„Ÿè¬ã•ã‚ŒãŸã“ã¨ï¼é ¼ã¾ã‚ŒãŸã“ã¨', ''),
                'not_bad_at': row.get('è‹¦æ‰‹ã˜ã‚ƒãªã„ã“ã¨ï¼ã¤ã„å¼•ãå—ã‘ã¦ã—ã¾ã†ã“ã¨', ''),
                'weaknesses': row.get('ã€Œã“ã‚Œã¯è‹¦æ‰‹...ã€ã¨æ€ã†ã“ã¨', '')
            },
            'submitted': row.get('æœ¬äººæå‡ºæ¸ˆ', '') == 'æ¸ˆ'
        }

        # URLã‹ã‚‰ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æƒ…å ±ã‚’å–å¾—
        if participant['profile_url']:
            profile_info = self.profile_extractor.extract_profile_from_url(participant['profile_url'])
            if profile_info:
                participant['profile_info'] = profile_info

        return participant

    def save_processed_data(self, participants: List[Dict[str, Any]]) -> None:
        """å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        try:
            output_data = {
                'processed_at': time.strftime('%Y-%m-%d %H:%M:%S'),
                'total_participants': len(participants),
                'participants': participants
            }

            if FileUtils.safe_write_json(output_data, config.PROCESSED_DATA_FILE):
                self.logger.info(f"å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {config.PROCESSED_DATA_FILE}")
                self.logger.info(f"å‚åŠ è€…æ•°: {len(participants)}äºº")
            else:
                self.logger.error("ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")

        except Exception as e:
            self.logger.error(f"ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def run(self) -> None:
        """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        self.logger.info("=== Skill-Zero Analyzer - Data Processor ===")
        self.logger.info("ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")

        # ãƒ‡ãƒãƒƒã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ•´ç†
        DebugFileCleaner.clean_debug_files()

        # CSVãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        df = self.load_csv_data()
        if df is None:
            return

        # å‚åŠ è€…ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
        participants = []
        for index, row in df.iterrows():
            participant = self.process_participant_data(row)
            if participant:
                participants.append(participant)
                self.logger.info(f"å‚åŠ è€…ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã¾ã—ãŸ: {participant['nickname'] or 'åå‰ãªã—'}")

        # é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ
        self.logger.info("é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆã‚’é–‹å§‹...")
        merged_participants = DataMerger.merge_duplicate_participants(participants)
        self.logger.info(f"çµ±åˆå‰: {len(participants)}äºº â†’ çµ±åˆå¾Œ: {len(merged_participants)}äºº")

        # å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        self.save_processed_data(merged_participants)

        self.logger.info("=== ãƒ‡ãƒ¼ã‚¿å‡¦ç†å®Œäº† ===")
        self.logger.info(f"å‡¦ç†ã•ã‚ŒãŸå‚åŠ è€…æ•°: {len(merged_participants)}äºº")
        self.logger.info(f"å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {config.PROCESSED_DATA_FILE}")


if __name__ == "__main__":
    processor = DataProcessor()
    processor.run()